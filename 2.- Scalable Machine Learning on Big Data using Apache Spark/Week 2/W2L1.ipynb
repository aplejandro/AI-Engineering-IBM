{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to run in a IBM Watson Studio default runtime (NOT the Watson Studio Apache Spark Runtime as the default runtime with 1 vCPU is free of charge). Therefore, we install Apache Spark in local mode for test purposes only. Please don't use it in production.\n",
    "\n",
    "In case you are facing issues, please read the following two documents first:\n",
    "\n",
    "<https://github.com/IBM/skillsnetwork/wiki/Environment-Setup>\n",
    "\n",
    "<https://github.com/IBM/skillsnetwork/wiki/FAQ>\n",
    "\n",
    "Then, please feel free to ask:\n",
    "\n",
    "[https://coursera.org/learn/machine-learning-big-data-apache-spark/discussions/all](https://coursera.org/learn/machine-learning-big-data-apache-spark/discussions/all?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0201EN-SkillsNetwork-20647446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n",
    "\n",
    "Please make sure to follow the guidelines before asking a question:\n",
    "\n",
    "<https://github.com/IBM/skillsnetwork/wiki/FAQ#im-feeling-lost-and-confused-please-help-me>\n",
    "\n",
    "If running outside Watson Studio, this should work as well. In case you are running in an Apache Spark context outside Watson Studio, please remove the Apache Spark setup in the first notebook cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
    "\n",
    "\n",
    "if ('sc' in locals() or 'sc' in globals()):\n",
    "    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: pyspark==2.4.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (2.4.5)\n",
      "Requirement already satisfied: py4j==0.10.7 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pyspark==2.4.5) (0.10.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==2.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Welcome to exercise one of week two of “Apache Spark for Scalable Machine Learning on BigData”. In this exercise you’ll read a DataFrame in order to perform a simple statistical analysis. Then you’ll rebalance the dataset. No worries, we’ll explain everything to you, let’s get started.\n",
    "\n",
    "Let’s create a data frame from a remote file by downloading it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-07 04:10:47--  https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n",
      "--2021-04-07 04:10:47--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n",
      "--2021-04-07 04:10:47--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 932997 (911K) [application/octet-stream]\n",
      "Saving to: ‘hmp.parquet’\n",
      "\n",
      "hmp.parquet         100%[===================>] 911.13K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-04-07 04:10:47 (55.2 MB/s) - ‘hmp.parquet’ saved [932997/932997]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# delete files from previous runs\n",
    "!rm -f hmp.parquet*\n",
    "\n",
    "# download the file containing the data in PARQUET format\n",
    "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
    "    \n",
    "# create a dataframe out of it\n",
    "df = spark.read.parquet('hmp.parquet')\n",
    "\n",
    "# register a corresponding query table\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a look at the data set first. This dataset contains sensor recordings from different movement activities as we will see in the next week’s lectures. X, Y and Z contain accelerometer sensor values whereas the class field contains information about which movement has been recorded. The source field is optional and can be used for data lineage since it contains the file name of the original file where the particular row was imported from.\n",
    "\n",
    "More details on the data set can be found here:\n",
    "<https://github.com/wchill/HMP_Dataset>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+\n",
      "|  x|  y|  z|              source|      class|\n",
      "+---+---+---+--------------------+-----------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 51| 33|Accelerometer-201...|Brush_teeth|\n",
      "| 20| 50| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n",
      "| 20| 51| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 18| 49| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 19| 48| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 16| 53| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 18| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 18| 51| 32|Accelerometer-201...|Brush_teeth|\n",
      "+---+---+---+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- y: integer (nullable = true)\n",
      " |-- z: integer (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classical classification data set. One thing we always do during data analysis is checking if the classes are balanced. In other words, if there are more or less the same number of example in each class. Let’s find out by a simple aggregation using SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|         class|count(1)|\n",
      "+--------------+--------+\n",
      "| Use_telephone|   15225|\n",
      "| Standup_chair|   25417|\n",
      "|      Eat_meat|   31236|\n",
      "|     Getup_bed|   45801|\n",
      "|   Drink_glass|   42792|\n",
      "|    Pour_water|   41673|\n",
      "|     Comb_hair|   23504|\n",
      "|          Walk|   92254|\n",
      "|  Climb_stairs|   40258|\n",
      "| Sitdown_chair|   25036|\n",
      "|   Liedown_bed|   11446|\n",
      "|Descend_stairs|   15375|\n",
      "|   Brush_teeth|   29829|\n",
      "|      Eat_soup|    6683|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select class,count(*) from df group by class').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there is quite an imbalance between classes. Before we dig into this, let’s re-write the same query using the DataFrame API – just in case you are not familiar with SQL. As we’ve learned before, it doesn’t matter if you express your queries with SQL or the DataFrame API – it all gets boiled down into the same execution plan optimized by Tungsten and accelerated by Catalyst. You can even mix and match SQL and DataFrame API code if you like.\n",
    "\n",
    "Again, more details on the API can be found here:\n",
    "[https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0201EN-SkillsNetwork-20647446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0201EN-SkillsNetwork-20647446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|         class|count|\n",
      "+--------------+-----+\n",
      "| Use_telephone|15225|\n",
      "| Standup_chair|25417|\n",
      "|      Eat_meat|31236|\n",
      "|     Getup_bed|45801|\n",
      "|   Drink_glass|42792|\n",
      "|    Pour_water|41673|\n",
      "|     Comb_hair|23504|\n",
      "|          Walk|92254|\n",
      "|  Climb_stairs|40258|\n",
      "| Sitdown_chair|25036|\n",
      "|   Liedown_bed|11446|\n",
      "|Descend_stairs|15375|\n",
      "|   Brush_teeth|29829|\n",
      "|      Eat_soup| 6683|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('class').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting pixiedust\n",
      "  Downloading pixiedust-1.1.19.tar.gz (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting geojson\n",
      "  Downloading geojson-2.5.0-py2.py3-none-any.whl (14 kB)\n",
      "Collecting astunparse\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: markdown in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pixiedust) (3.1.1)\n",
      "Collecting colour\n",
      "  Downloading colour-0.1.5-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pixiedust) (2.24.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pixiedust) (3.2.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pixiedust) (1.0.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from astunparse->pixiedust) (0.34.2)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from astunparse->pixiedust) (1.15.0)\n",
      "Requirement already satisfied: setuptools>=36 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from markdown->pixiedust) (47.3.1.post20200622)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->pixiedust) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->pixiedust) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->pixiedust) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->pixiedust) (1.25.9)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib->pixiedust) (1.18.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib->pixiedust) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib->pixiedust) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib->pixiedust) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib->pixiedust) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas->pixiedust) (2020.1)\n",
      "Building wheels for collected packages: pixiedust\n",
      "  Building wheel for pixiedust (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pixiedust: filename=pixiedust-1.1.19-py3-none-any.whl size=321803 sha256=f62490a8cf17233bfbe78eec3a05c86d208da9043ac90e1cd82fc8be9f5055ac\n",
      "  Stored in directory: /tmp/wsuser/.cache/pip/wheels/05/07/e7/8aca0e820027a63157a916424fd748fb2a2a3e71de5e08eeb8\n",
      "Successfully built pixiedust\n",
      "Installing collected packages: geojson, astunparse, colour, pixiedust\n",
      "Successfully installed astunparse-1.6.3 colour-0.1.5 geojson-2.5.0 pixiedust-1.1.19\n"
     ]
    }
   ],
   "source": [
    "!pip install pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create a bar plot from this data. We’re using the pixidust library, which is Open Source, because of its simplicity. But any other library like matplotlib is fine as well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "barChart",
      "keyFields": "class",
      "legend": "true",
      "mpld3": "false",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "sortby": "Values ASC",
      "valueFields": "count"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n",
      "Table VERSION_TRACKER created successfully\n",
      "Table METRICS_TRACKER created successfully\n",
      "\n",
      "Share anonymous install statistics? (opt-out instructions)\n",
      "\n",
      "PixieDust will record metadata on its environment the next time the package is installed or updated. The data is anonymized and aggregated to help plan for future releases, and records only the following values:\n",
      "\n",
      "{\n",
      "   \"data_sent\": currentDate,\n",
      "   \"runtime\": \"python\",\n",
      "   \"application_version\": currentPixiedustVersion,\n",
      "   \"space_id\": nonIdentifyingUniqueId,\n",
      "   \"config\": {\n",
      "       \"repository_id\": \"https://github.com/ibm-watson-data-lab/pixiedust\",\n",
      "       \"target_runtimes\": [\"Data Science Experience\"],\n",
      "       \"event_id\": \"web\",\n",
      "       \"event_organizer\": \"dev-journeys\"\n",
      "   }\n",
      "}\n",
      "You can opt out by calling pixiedust.optOut() in a new cell.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.19</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPixiedust runtime updated. Please restart kernel\u001b[0m\n",
      "Table SPARK_PACKAGES created successfully\n",
      "Table USER_PREFERENCES created successfully\n",
      "Table service_connections created successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[class: string, count: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pixiedust\n",
    "from pyspark.sql.functions import col\n",
    "counts = df.groupBy('class').count().orderBy('count')\n",
    "display(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks nice, but it would be nice if we can aggregate further to obtain some quantitative metrics on the imbalance like, min, max, mean and standard deviation. If we divide max by min we get a measure called minmax ration which tells us something about the relationship between the smallest and largest class. Again, let’s first use SQL for those of you familiar with SQL. Don’t be scared, we’re used nested sub-selects, basically selecting from a result of a SQL query like it was a table. All within on SQL statement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+------------------+-----------------+\n",
      "| min|  max|              mean|            stddev|      minmaxratio|\n",
      "+----+-----+------------------+------------------+-----------------+\n",
      "|6683|92254|31894.928571428572|21284.893716741157|13.80427951518779|\n",
      "+----+-----+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select \n",
    "        *,\n",
    "        max/min as minmaxratio -- compute minmaxratio based on previously computed values\n",
    "        from (\n",
    "            select \n",
    "                min(ct) as min, -- compute minimum value of all classes\n",
    "                max(ct) as max, -- compute maximum value of all classes\n",
    "                mean(ct) as mean, -- compute mean between all classes\n",
    "                stddev(ct) as stddev -- compute standard deviation between all classes\n",
    "                from (\n",
    "                    select\n",
    "                        count(*) as ct -- count the number of rows per class and rename it to ct\n",
    "                        from df -- access the temporary query table called df backed by DataFrame df\n",
    "                        group by class -- aggrecate over class\n",
    "                )\n",
    "        )   \n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same query can be expressed using the DataFrame API. Again, don’t be scared. It’s just a sequential expression of transformation steps. You now an choose which syntax you like better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+------------------+-----------------+\n",
      "| min|  max|              mean|            stddev|      minmaxratio|\n",
      "+----+-----+------------------+------------------+-----------------+\n",
      "|6683|92254|31894.928571428572|21284.893716741157|13.80427951518779|\n",
      "+----+-----+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, min, max, mean, stddev\n",
    "\n",
    "df \\\n",
    "    .groupBy('class') \\\n",
    "    .count() \\\n",
    "    .select([ \n",
    "        min(col(\"count\")).alias('min'), \n",
    "        max(col(\"count\")).alias('max'), \n",
    "        mean(col(\"count\")).alias('mean'), \n",
    "        stddev(col(\"count\")).alias('stddev') \n",
    "    ]) \\\n",
    "    .select([\n",
    "        col('*'),\n",
    "        (col(\"max\") / col(\"min\")).alias('minmaxratio')\n",
    "    ]) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time for you to work on the data set. First, please create a table of all classes with the respective counts, but this time, please order the table by the count number, ascending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|         class|count|\n",
      "+--------------+-----+\n",
      "|          Walk|92254|\n",
      "|     Getup_bed|45801|\n",
      "|   Drink_glass|42792|\n",
      "|    Pour_water|41673|\n",
      "|  Climb_stairs|40258|\n",
      "|      Eat_meat|31236|\n",
      "|   Brush_teeth|29829|\n",
      "| Standup_chair|25417|\n",
      "| Sitdown_chair|25036|\n",
      "|     Comb_hair|23504|\n",
      "|Descend_stairs|15375|\n",
      "| Use_telephone|15225|\n",
      "|   Liedown_bed|11446|\n",
      "|      Eat_soup| 6683|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('class').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixiedust is a very sophisticated library. It takes care of sorting as well. Please modify the bar chart so that it gets sorted by the number of elements per class, ascending. Hint: It’s an option available in the UI once rendered using the display() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[class: string, count: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### your code goes here\n",
    "import pixiedust\n",
    "from pyspark.sql.functions import col\n",
    "counts = df.groupBy('class').count().orderBy('count')\n",
    "display(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced classes can cause pain in machine learning. Therefore let’s rebalance. In the flowing we limit the number of elements per class to the amount of the least represented class. This is called undersampling. Other ways of rebalancing can be found here:\n",
    "\n",
    "[https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0201EN-SkillsNetwork-20647446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min\n",
    "\n",
    "# create a lot of distinct classes from the dataset\n",
    "classes = [row[0] for row in df.select('class').distinct().collect()]\n",
    "\n",
    "# compute the number of elements of the smallest class in order to limit the number of samples per calss\n",
    "min = df.groupBy('class').count().select(min('count')).first()[0]\n",
    "\n",
    "# define the result dataframe variable\n",
    "df_balanced = None\n",
    "\n",
    "# iterate over distinct classes\n",
    "for cls in classes:\n",
    "    \n",
    "    # only select examples for the specific class within this iteration\n",
    "    # shuffle the order of the elements (by setting fraction to 1.0 sample works like shuffle)\n",
    "    # return only the first n samples\n",
    "    df_temp = df \\\n",
    "        .filter(\"class = '\"+cls+\"'\") \\\n",
    "        .sample(False, 1.0) \\\n",
    "        .limit(min)\n",
    "    \n",
    "    # on first iteration, assing df_temp to empty df_balanced\n",
    "    if df_balanced == None:    \n",
    "        df_balanced = df_temp\n",
    "    # afterwards, append vertically\n",
    "    else:\n",
    "        df_balanced=df_balanced.union(df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please verify, by using the code cell below, if df_balanced has the same number of elements per class. You should get 6683 elements per class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|         class|count|\n",
      "+--------------+-----+\n",
      "| Use_telephone| 6683|\n",
      "| Standup_chair| 6683|\n",
      "|      Eat_meat| 6683|\n",
      "|     Getup_bed| 6683|\n",
      "|   Drink_glass| 6683|\n",
      "|    Pour_water| 6683|\n",
      "|     Comb_hair| 6683|\n",
      "|          Walk| 6683|\n",
      "|  Climb_stairs| 6683|\n",
      "| Sitdown_chair| 6683|\n",
      "|   Liedown_bed| 6683|\n",
      "|Descend_stairs| 6683|\n",
      "|   Brush_teeth| 6683|\n",
      "|      Eat_soup| 6683|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_balanced.groupBy('class').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by <a href=\"https://linkedin.com/in/romeo-kienzler-089b4557\"> Romeo Kienzler </a>  I hope you found this lab interesting and educational. Feel free to contact me if you have any questions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
    "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
    "| 2020-09-29        | 2.0     | Srishti    | Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
