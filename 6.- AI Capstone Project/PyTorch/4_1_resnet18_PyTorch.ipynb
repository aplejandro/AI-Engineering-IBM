{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "4.1_resnet18_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f4cb8602de4c465a9e76b64ccb4cf67f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_494cf0f7f7904249bd989893b91c72a6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ec204658a6e6467fa3a0a3380536817b",
              "IPY_MODEL_44b54f09f6a84e088e96cc5cffbd200a"
            ]
          }
        },
        "494cf0f7f7904249bd989893b91c72a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec204658a6e6467fa3a0a3380536817b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_635108ee0e88400ea63c56cdc639ea6f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_23205d8e84884dedb7e89806af1fcaa9"
          }
        },
        "44b54f09f6a84e088e96cc5cffbd200a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a4d06fe527734e8e885bca2af6c19938",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [02:51&lt;00:00, 274kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_10ab451fb8ec47769ab186c26b4fa3ec"
          }
        },
        "635108ee0e88400ea63c56cdc639ea6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "23205d8e84884dedb7e89806af1fcaa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4d06fe527734e8e885bca2af6c19938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "10ab451fb8ec47769ab186c26b4fa3ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFF_lBo5bvGd"
      },
      "source": [
        "<a href=\"http://cocl.us/pytorch_link_top\">\n",
        "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
        "</a> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4DoAhe0bvGi"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hvX1zZrbvGj"
      },
      "source": [
        "<h1><h1>Pre-trained-Models with PyTorch </h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqObjy4abvGj"
      },
      "source": [
        "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n",
        "\n",
        "<ul>\n",
        "<li>change the output layer</li>\n",
        "<li> train the model</li> \n",
        "<li>  identify  several  misclassified samples</li> \n",
        " </ul>\n",
        "You will take several screenshots of your work and share your notebook. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr1oevYhbvGk"
      },
      "source": [
        "<h2>Table of Contents</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNYnfaIIbvGk"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"#download_data\"> Download Data</a></li>\n",
        "    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
        "    <li><a href=\"#data_class\"> Dataset Class</a></li>\n",
        "    <li><a href=\"#Question_1\">Question 1</a></li>\n",
        "    <li><a href=\"#Question_2\">Question 2</a></li>\n",
        "    <li><a href=\"#Question_3\">Question 3</a></li>\n",
        "</ul>\n",
        "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
        " </div>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uU_YGw_bvGl"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_m39KRtbvGl"
      },
      "source": [
        "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F19KmsinbvGm",
        "outputId": "e2be5253-8073-4c29-8ac8-cfb72d702f87"
      },
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-26 02:52:04--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2598656062 (2.4G) [application/zip]\n",
            "Saving to: ‘Positive_tensors.zip’\n",
            "\n",
            "Positive_tensors.zi 100%[===================>]   2.42G  36.3MB/s    in 72s     \n",
            "\n",
            "2021-04-26 02:53:17 (34.3 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnynfKRnbvGm"
      },
      "source": [
        "!unzip -q Positive_tensors.zip "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGcrJ3KebvGm",
        "outputId": "51a3515c-1e3e-4da1-d80a-a35952bb8ca0"
      },
      "source": [
        "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
        "!unzip -q Negative_tensors.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-26 02:56:21--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2111408108 (2.0G) [application/zip]\n",
            "Saving to: ‘Negative_tensors.zip’\n",
            "\n",
            "Negative_tensors.zi 100%[===================>]   1.97G  39.2MB/s    in 55s     \n",
            "\n",
            "2021-04-26 02:57:17 (36.5 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX6tmI7-bvGn"
      },
      "source": [
        "We will install torchvision:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1a0ySzEbvGn",
        "outputId": "04bcf43c-72f1-4d12-8e1e-c2cfa04ad519"
      },
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKcs_wSxbvGn"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdCCcQFFbvGo"
      },
      "source": [
        "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFSfivPdbvGo",
        "outputId": "14d39129-3d48-40c9-f395-3d49c1eb5d4a"
      },
      "source": [
        "# These are the libraries will be used for this lab.\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch \n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9647a003d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbdeId4_bvGo"
      },
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thQKZCFQbvGp"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJQn8AKubvGp"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya3JpFUebvGp"
      },
      "source": [
        " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGdYKo2ibvGq",
        "outputId": "5198831c-e173-4e65-fe58-1f5c0fa389b6"
      },
      "source": [
        "# Create your own dataset object\n",
        "\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\"/content/\"\n",
        "        positive=\"Positive_tensors\"\n",
        "        negative='Negative_tensors'\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files \n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "        \n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:30000]\n",
        "            self.Y=self.Y[0:30000]\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)     \n",
        "       \n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "               \n",
        "        image=torch.load(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "                  \n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y\n",
        "    \n",
        "print(\"done\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0k8o7lVbvGr"
      },
      "source": [
        "We create two dataset objects, one for the training data and one for the validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqEZBpgYbvGr",
        "outputId": "54cde67f-f3c5-48df-8de0-f6eedad2d4d1"
      },
      "source": [
        "train_dataset = Dataset(train=True)\n",
        "validation_dataset = Dataset(train=False)\n",
        "print(\"done\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCf1KNudbvGs"
      },
      "source": [
        "<h2 id=\"Question_1\">Question 1</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8tbbdWFbvGs"
      },
      "source": [
        "<b>Prepare a pre-trained resnet18 model :</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QM8SV5fbvGs"
      },
      "source": [
        "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "f4cb8602de4c465a9e76b64ccb4cf67f",
            "494cf0f7f7904249bd989893b91c72a6",
            "ec204658a6e6467fa3a0a3380536817b",
            "44b54f09f6a84e088e96cc5cffbd200a",
            "635108ee0e88400ea63c56cdc639ea6f",
            "23205d8e84884dedb7e89806af1fcaa9",
            "a4d06fe527734e8e885bca2af6c19938",
            "10ab451fb8ec47769ab186c26b4fa3ec"
          ]
        },
        "id": "I-sZL-awbvGs",
        "outputId": "fb39b785-828f-4ca2-c2d4-c8b9ef8ab9dd"
      },
      "source": [
        "# Step 1: Load the pre-trained model resnet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "# Type your code here"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4cb8602de4c465a9e76b64ccb4cf67f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhZy9uIebvGt"
      },
      "source": [
        "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEY4y0asbvGt"
      },
      "source": [
        "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "# Type your code here"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O2X_NmtbvGt"
      },
      "source": [
        "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xnAJcJwbvGu"
      },
      "source": [
        "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci8tWrbNbvGu"
      },
      "source": [
        "model.fc = nn.Linear(512, 2)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWxMgNp1bvGu"
      },
      "source": [
        "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZLAgrWVbvGu",
        "outputId": "34989b4a-5e54-473a-c93c-ee0aa0fe385c"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYc10ISYbvGv"
      },
      "source": [
        "<h2 id=\"Question_2\">Question 2: Train the Model</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfeyocWDbvGv"
      },
      "source": [
        "In this question you will train your, model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPAlN_kpbvGv"
      },
      "source": [
        "<b>Step 1</b>: Create a cross entropy criterion function \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drvYX4YQbvGv"
      },
      "source": [
        "# Step 1: Create the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Type your code here"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIMq8RkYbvGv"
      },
      "source": [
        "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7kIk3yXbvGw"
      },
      "source": [
        "batch_size = 500\n",
        "trainloader = DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
        "validloader = DataLoader(dataset=validation_dataset, batch_size=batch_size)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_yqrgX5bvGw"
      },
      "source": [
        "<b>Step 3</b>: Use the following optimizer to minimize the loss \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D6y323obvGw"
      },
      "source": [
        "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLUDLNlobvGw"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDhfEWQ3bvGw"
      },
      "source": [
        "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILA0Pe12bvGx",
        "outputId": "3d749702-6ce0-48e1-8d0d-d18d17a6544b"
      },
      "source": [
        "n_epochs=1\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "correct=0\n",
        "N_test=len(validation_dataset)\n",
        "N_train=len(train_dataset)\n",
        "start_time = time.time()\n",
        "#n_epochs\n",
        "\n",
        "Loss=0\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for x, y in trainloader:\n",
        "        print(\"One more done\")\n",
        "        model.train() \n",
        "        optimizer.zero_grad()\n",
        "        yhat = model(x)\n",
        "        loss = criterion(yhat, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_list.append(loss.data)\n",
        "    correct=0\n",
        "    for x_test, y_test in validloader:\n",
        "        model.eval()\n",
        "        z = model(x_test)\n",
        "        _, yhat = torch.max(z.data, 1)\n",
        "        correct += (yhat == y_test).sum().item()\n",
        "    accuracy=correct/N_test"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n",
            "One more done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsfjVP-HbvGx"
      },
      "source": [
        "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lhnqo3XubvGy",
        "outputId": "400eeec0-e468-4731-9244-61df790bf05d"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9891"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Lm7f5nWObvGy",
        "outputId": "a055ea94-b9ed-45be-c15d-4a528ee6a77d"
      },
      "source": [
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C93SPTv1bvGy"
      },
      "source": [
        "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYsDNzpdbvGy"
      },
      "source": [
        "<b>Identify the first four misclassified samples using the validation data:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7cg87_DbvGz"
      },
      "source": [
        "validloader = DataLoader(dataset=validation_dataset, batch_size=1)\n",
        "model.eval()\n",
        "counter = 0\n",
        "sample_idx = []\n",
        "idx = 0\n",
        "for x, y in validloader:\n",
        "    idx += 1\n",
        "    z = model(x)\n",
        "    _, yhat = torch.max(z.data, 1)\n",
        "    if yhat != y:\n",
        "      counter += 1\n",
        "      sample_idx.append([idx, yhat, y])\n",
        "    if counter > 3:\n",
        "      break"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9FjaDcPFtdd",
        "outputId": "c91ea6c4-3125-4edb-feaf-4daaad0d4571"
      },
      "source": [
        "for i in sample_idx:\n",
        "    print(f\"Sample : {i[0]}, predicted value: {i[1]}, actual value: {i[2]}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample : 7, predicted value: tensor([0]), actual value: tensor([1])\n",
            "Sample : 355, predicted value: tensor([0]), actual value: tensor([1])\n",
            "Sample : 387, predicted value: tensor([0]), actual value: tensor([1])\n",
            "Sample : 405, predicted value: tensor([0]), actual value: tensor([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxAu9cBJFw_d"
      },
      "source": [
        "full_model_path = \"full_pytorch_model\"\n",
        "torch.save(model, full_model_path)\n",
        "state_dict_path = \"pytorch_state_dict\"\n",
        "torch.save(model.state_dict(), state_dict_path)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJqDBraJbvGz"
      },
      "source": [
        "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html\"> CLICK HERE </a> Click here to see how to share your notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4O1H9uWbvGz"
      },
      "source": [
        "<h2>About the Authors:</h2> \n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E_6GwjQbvGz"
      },
      "source": [
        "## Change Log\n",
        "\n",
        "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
        "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
        "| 2020-09-21        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n",
        "\n",
        "<hr>\n",
        "\n",
        "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r85iUWwbvG0"
      },
      "source": [
        "Copyright © 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqw4EsHEF1Ik"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}