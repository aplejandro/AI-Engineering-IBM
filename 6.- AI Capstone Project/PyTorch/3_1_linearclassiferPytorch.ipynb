{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "3.1_linearclassiferPytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDa07VOaNaVO"
      },
      "source": [
        "<a href=\"http://cocl.us/pytorch_link_top\">\n",
        "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
        "</a> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNEkJb4XNaVY"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZnl_F_wNaVZ"
      },
      "source": [
        "<h1>Objective</h1><ul><li> How to use linear classifier in pytorch.</li></ul> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYNULzDNNaVa"
      },
      "source": [
        "<h1>Linear  Classifier with PyTorch </h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mFMFO2kNaVa"
      },
      "source": [
        "<p>Before you use a  Deep neural network to solve the classification problem,  it 's a good idea to try and solve the problem with the simplest method. You will need the dataset object from the previous section.\n",
        "In this lab, we solve the problem with a linear classifier.\n",
        " You will be asked to determine the maximum accuracy your linear classifier can achieve on the validation data for 5 epochs. We will give some free parameter values if you follow the instructions you will be able to answer the quiz. Just like the other labs there are several steps, but in this lab you will only be quizzed on the final result. </p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q-Z83RoNaVb"
      },
      "source": [
        "<h2>Table of Contents</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPwOWMikNaVc"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"#download_data\"> Download data</a></li>\n",
        "    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
        "    <li><a href=\"#data_class\"> Dataset Class</a></li>\n",
        "    <li><a href=\"#trasform_Data_object\">Transform Object and Dataset Object</a></li>\n",
        "    <li><a href=\"#Question\">Question</a></li>\n",
        "</ul>\n",
        "<p>Estimated Time Needed: <strong>25 min</strong></p>\n",
        " </div>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgV6-sJoNaVc"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eix3yKHqNaVd"
      },
      "source": [
        "In this section, you are going to download the data from IBM object storage using <b>wget</b>, then unzip them.  <b>wget</b> is a command the retrieves content from web servers, in this case its a zip file. Locally we store the data in the directory  <b>/resources/data</b> . The <b>-p</b> creates the entire directory tree up to the given directory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIgV6RW4NaVd"
      },
      "source": [
        "First, we download the file that contains the images, if you dint do this in your first lab uncomment:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFm2Ps-bNaVe",
        "outputId": "e6286641-9f6a-4caf-9541-27d131a60f28"
      },
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip -P /resources/data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-26 17:26:17--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 245259777 (234M) [application/zip]\n",
            "Saving to: ‘/resources/data/concrete_crack_images_for_classification.zip’\n",
            "\n",
            "concrete_crack_imag 100%[===================>] 233.90M  34.8MB/s    in 7.0s    \n",
            "\n",
            "2021-04-26 17:26:25 (33.4 MB/s) - ‘/resources/data/concrete_crack_images_for_classification.zip’ saved [245259777/245259777]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSjgwPSDNaVf"
      },
      "source": [
        "We then unzip the file, this ma take a while:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tsu7g0fuNaVf",
        "outputId": "5b17ef5e-4526-4776-8553-104d7273fccd"
      },
      "source": [
        "!unzip -q  /resources/data/concrete_crack_images_for_classification.zip -d  /resources/data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace /resources/data/Negative/18476.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /resources/data/Negative/00189.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /resources/data/Negative/04111.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAu1u83nNaVg"
      },
      "source": [
        "We then download the files that contain the negative images:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fJc7AQuNaVg"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MdO6IGkNaVg"
      },
      "source": [
        "The following are the libraries we are going to use for this lab:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBHVDiZjNaVh"
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torch import optim "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYumKPi-NaVh"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNvAKQYbNaVh"
      },
      "source": [
        "In this section, we will use the previous code to build a dataset class. As before, make sure the even samples are positive, and the odd samples are negative.  If the parameter <code>train</code> is set to <code>True</code>, use the first 30 000  samples as training data; otherwise, the remaining samples will be used as validation data. Do not forget to sort your files so they are in the same order.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t95bvm7NaVi"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\"/resources/data\"\n",
        "        positive=\"Positive\"\n",
        "        negative=\"Negative\"\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in  os.listdir(positive_file_path) if file.endswith(\".jpg\")]\n",
        "        positive_files.sort()\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in  os.listdir(negative_file_path) if file.endswith(\".jpg\")]\n",
        "        negative_files.sort()\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files \n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "        \n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:30000]\n",
        "            self.Y=self.Y[0:30000]\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)    \n",
        "       \n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        \n",
        "        image=Image.open(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "          \n",
        "        \n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxQhqTKpNaVj"
      },
      "source": [
        "<h2 id=\"trasform_Data_object\">Transform Object and Dataset Object</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-mCzhuBNaVk"
      },
      "source": [
        "Create a transform object, that uses the <code>Compose</code> function. First use the transform <code>ToTensor()</code> and followed by <code>Normalize(mean, std)</code>. The value for <code> mean</code> and <code>std</code> are provided for you.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UtueYnENaVk"
      },
      "source": [
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "# transforms.ToTensor()\n",
        "#transforms.Normalize(mean, std)\n",
        "#transforms.Compose([])\n",
        "\n",
        "transform =transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean, std)])\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wxrF9gRNaVl"
      },
      "source": [
        "Create object for the training data  <code>dataset_train</code> and validation <code>dataset_val</code>. Use the transform object to convert the images to tensors using the transform object:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm-OBCQRNaVl"
      },
      "source": [
        "dataset_train=Dataset(transform=transform,train=True)\n",
        "dataset_val=Dataset(transform=transform,train=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyqcEF3aNaVm"
      },
      "source": [
        "We  can find the shape of the image:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arTSt0jlNaVm",
        "outputId": "dec20b43-62d4-4763-be47-5a5c414ac928"
      },
      "source": [
        "dataset_train[0][0].shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 227, 227])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW-1Se5zNaVn"
      },
      "source": [
        "We see that it's a color image with three channels:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj7P79flNaVn",
        "outputId": "6ba80078-204a-44a0-f753-9ef1efef756a"
      },
      "source": [
        "size_of_image=3*227*227\n",
        "size_of_image"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "154587"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCnxTYJmNaVn"
      },
      "source": [
        "<h2 id=\"Question\"> Question <h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulqk-CPgNaVn"
      },
      "source": [
        "<b> Create a custom module for Softmax for two classes,called model. The input size should be the <code>size_of_image</code>, you should record the maximum accuracy achieved on the validation data for the different epochs. For example if the 5 epochs the accuracy was 0.5, 0.2, 0.64,0.77, 0.66 you would select 0.77.</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0ys1tHXNaVo"
      },
      "source": [
        "Train the model with the following free parameter values:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIgz1AV9NaVo"
      },
      "source": [
        "<b>Parameter Values</b>\n",
        "\n",
        "   <li>learning rate:0.1 </li>\n",
        "   <li>momentum term:0.1 </li>\n",
        "   <li>batch size training:1000</li>\n",
        "   <li>Loss function:Cross Entropy Loss </li>\n",
        "   <li>epochs:5</li>\n",
        "   <li>set: torch.manual_seed(0)</li>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1HHJqtXNaVo",
        "outputId": "9ca7a148-364d-4ecf-b3f3-1407bb17b13d"
      },
      "source": [
        "torch.manual_seed(0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8ad025aad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9xuT_q3NaVp"
      },
      "source": [
        "<b>Custom Module:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B93u2iWNaVp"
      },
      "source": [
        "class SoftMax(nn.Module):\n",
        "\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(SoftMax, self).__init__()\n",
        "        self.linear = nn.Linear(in_size, out_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f85RYNpIPu-H"
      },
      "source": [
        "class CNN_batch(nn.Module):\n",
        "    \n",
        "    # Contructor\n",
        "    def __init__(self, out_1=32, out_2=64, number_of_classes=2):\n",
        "        super(CNN_batch, self).__init__()\n",
        "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=out_1, kernel_size=7, stride=3)\n",
        "        self.conv1_bn = nn.BatchNorm2d(out_1)\n",
        "\n",
        "        self.maxpool1=nn.MaxPool2d(kernel_size=3)\n",
        "        \n",
        "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=7, stride=3)\n",
        "        self.conv2_bn = nn.BatchNorm2d(out_2)\n",
        "\n",
        "        self.maxpool2=nn.MaxPool2d(kernel_size=3)\n",
        "        self.fc1 = nn.Linear(out_2 * 4, number_of_classes)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(number_of_classes)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "    \n",
        "    # Prediction\n",
        "    def forward(self, x):\n",
        "        x = self.cnn1(x)\n",
        "        x = self.conv1_bn(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.cnn2(x)\n",
        "        x = self.conv2_bn(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdJ5EH8PNaVp"
      },
      "source": [
        "<b>Model Object:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPB4F7kpNaVp",
        "outputId": "8b867b3b-3093-4823-b5db-ec29804f0d32"
      },
      "source": [
        "model = CNN_batch()\n",
        "model.parameters\n",
        "#model = SoftMax(size_of_image, 2)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of CNN_batch(\n",
              "  (cnn1): Conv2d(3, 32, kernel_size=(7, 7), stride=(3, 3))\n",
              "  (conv1_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (maxpool1): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
              "  (cnn2): Conv2d(32, 64, kernel_size=(7, 7), stride=(3, 3))\n",
              "  (conv2_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (maxpool2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=256, out_features=2, bias=True)\n",
              "  (bn_fc1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (softmax): Softmax(dim=-1)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJKHZWX7NaVq"
      },
      "source": [
        "<b>Optimizer:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQlRPcgRNaVq"
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.1)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKq6rPTcNaVq"
      },
      "source": [
        "<b>Criterion:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQlHFonnNaVq"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo8oWQfzNaVq"
      },
      "source": [
        "<b>Data Loader Training and Validation:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjWe-o_hNaVr"
      },
      "source": [
        "batch_size = 100\n",
        "trainloader = DataLoader(dataset=dataset_train, batch_size=batch_size)\n",
        "validloader = DataLoader(dataset=dataset_val, batch_size=batch_size)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYEcRkoxNaVr"
      },
      "source": [
        "<b>Train Model with 5 epochs, should take 35 minutes: </b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGt0AoabNaVr",
        "outputId": "4e1f60a5-6436-4ba2-e1c3-ad23b17b7a70"
      },
      "source": [
        "epochs_n = 5\n",
        "\n",
        "def train_model(epochs):\n",
        "  LOSS = []\n",
        "  val_acc = []\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"epoch: {epoch}\")\n",
        "    for x, y in trainloader:\n",
        "      model.train()\n",
        "      optimizer.zero_grad()\n",
        "      #yhat = model(x.view(-1, size_of_image))\n",
        "      yhat = model(x)\n",
        "      loss = criterion(yhat, y)\n",
        "      LOSS.append(loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print(\"Validation\")\n",
        "    correct=0\n",
        "    for x_test, y_test in validloader:\n",
        "      model.eval()\n",
        "      #z = model(x_test.view(-1, size_of_image))\n",
        "      z = model(x_test)\n",
        "      _, yhat = torch.max(z.data, 1)\n",
        "      correct += (yhat == y_test).sum().item()\n",
        "    accuracy = correct / len(dataset_val)\n",
        "    val_acc.append(accuracy)\n",
        "  return LOSS, val_acc\n",
        "\n",
        "\n",
        "loss, val_acc = train_model(epochs_n)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0\n",
            "Validation\n",
            "epoch: 1\n",
            "Validation\n",
            "epoch: 2\n",
            "Validation\n",
            "epoch: 3\n",
            "Validation\n",
            "epoch: 4\n",
            "Validation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "lUtYxK1iP4wK",
        "outputId": "05a414ef-7f62-48f0-986e-a35b28abc9a1"
      },
      "source": [
        "plt.plot(loss)\n",
        "val_acc"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9537, 0.958, 0.9667, 0.955, 0.9863]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpPtSoyvP6jh"
      },
      "source": [
        "full_model_path = \"full_pytorch_model\"\n",
        "torch.save(model, full_model_path)\n",
        "state_dict_path = \"pytorch_state_dict\"\n",
        "torch.save(model.state_dict(), state_dict_path)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oWRpZYjUnE5",
        "outputId": "a7044bdb-15d9-4c89-c0d4-9e58eb8d85e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f69BwIR4Ugir"
      },
      "source": [
        "!cp \"/content/full_pytorch_model\" \"/content/gdrive/MyDrive\"\n",
        "!cp \"/content/pytorch_state_dict\" \"/content/gdrive/MyDrive\""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "modHr1zwZMBj",
        "outputId": "eded4bb5-0178-4853-e2f5-e231786e7c3c"
      },
      "source": [
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "cnn1.weight \t torch.Size([32, 3, 7, 7])\n",
            "cnn1.bias \t torch.Size([32])\n",
            "conv1_bn.weight \t torch.Size([32])\n",
            "conv1_bn.bias \t torch.Size([32])\n",
            "conv1_bn.running_mean \t torch.Size([32])\n",
            "conv1_bn.running_var \t torch.Size([32])\n",
            "conv1_bn.num_batches_tracked \t torch.Size([])\n",
            "cnn2.weight \t torch.Size([64, 32, 7, 7])\n",
            "cnn2.bias \t torch.Size([64])\n",
            "conv2_bn.weight \t torch.Size([64])\n",
            "conv2_bn.bias \t torch.Size([64])\n",
            "conv2_bn.running_mean \t torch.Size([64])\n",
            "conv2_bn.running_var \t torch.Size([64])\n",
            "conv2_bn.num_batches_tracked \t torch.Size([])\n",
            "fc1.weight \t torch.Size([2, 256])\n",
            "fc1.bias \t torch.Size([2])\n",
            "bn_fc1.weight \t torch.Size([2])\n",
            "bn_fc1.bias \t torch.Size([2])\n",
            "bn_fc1.running_mean \t torch.Size([2])\n",
            "bn_fc1.running_var \t torch.Size([2])\n",
            "bn_fc1.num_batches_tracked \t torch.Size([])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCRh6JqnP7M0",
        "outputId": "3a874de6-d595-4ca1-f6fd-8dad522b0e2f"
      },
      "source": [
        "model1 = CNN_batch()\n",
        "model1.load_state_dict(torch.load(state_dict_path))\n",
        "model1.eval()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN_batch(\n",
              "  (cnn1): Conv2d(3, 32, kernel_size=(7, 7), stride=(3, 3))\n",
              "  (conv1_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (maxpool1): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
              "  (cnn2): Conv2d(32, 64, kernel_size=(7, 7), stride=(3, 3))\n",
              "  (conv2_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (maxpool2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=256, out_features=2, bias=True)\n",
              "  (bn_fc1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (softmax): Softmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvnQWrPjP8vH",
        "outputId": "cdce1f5c-2ed0-400c-a914-ffa3b6f8ed15"
      },
      "source": [
        "for x, y in trainloader:\n",
        "  z = model1(x)\n",
        "  _, yhat = torch.max(z.data, 1)\n",
        "  correct = (yhat == y).sum().item()\n",
        "  print(correct, end=\", \")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100, 100, 98, 100, 99, 98, 99, 98, 100, 98, 98, 99, 99, 100, 100, 98, 99, 99, 100, 99, 97, 98, 100, 100, 99, 100, 99, 99, 99, 100, 98, 98, 99, 99, 100, 99, 99, 98, 99, 99, 99, 98, 99, 98, 99, 100, 99, 100, 99, 99, 99, 100, 100, 99, 98, 99, 99, 99, 97, 98, 98, 98, 99, 99, 96, 97, 98, 97, 100, 99, 99, 99, 98, 97, 100, 100, 100, 98, 99, 96, 99, 100, 100, 100, 99, 99, 97, 99, 98, 100, 98, 99, 100, 100, 97, 99, 100, 97, 98, 99, 98, 98, 97, 100, 100, 100, 97, 98, 99, 99, 99, 99, 100, 98, 98, 97, 99, 99, 99, 100, 98, 100, 99, 99, 99, 99, 98, 100, 98, 99, 97, 100, 98, 99, 100, 99, 99, 98, 96, 99, 96, 100, 100, 97, 100, 100, 100, 98, 100, 98, 98, 100, 98, 100, 98, 99, 100, 98, 95, 98, 99, 100, 99, 99, 98, 99, 98, 98, 100, 97, 97, 99, 97, 98, 100, 96, 100, 98, 97, 99, 100, 98, 100, 98, 98, 98, 98, 97, 100, 100, 99, 98, 97, 100, 100, 98, 98, 100, 97, 99, 98, 99, 100, 100, 99, 98, 99, 100, 100, 99, 100, 100, 100, 99, 100, 97, 98, 100, 99, 100, 100, 100, 99, 99, 100, 100, 98, 98, 99, 99, 99, 100, 99, 99, 100, 99, 100, 98, 100, 98, 99, 100, 99, 100, 97, 100, 98, 99, 99, 100, 97, 98, 96, 97, 100, 98, 99, 100, 100, 98, 100, 98, 99, 97, 100, 98, 98, 97, 100, 100, 100, 97, 100, 98, 100, 99, 97, 99, 99, 97, 100, 99, 98, 100, 97, 100, 100, 100, 96, 99, 99, 98, 100, 100, 100, 100, 95, 99, 98, 99, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2Qyh6vrNaVr"
      },
      "source": [
        "<h2>About the Authors:</h2>\n",
        " <a href=\\\"https://www.linkedin.com/in/joseph-s-50398b136/\\\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqj5xkmPNaVr"
      },
      "source": [
        "## Change Log\n",
        "\n",
        "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
        "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
        "| 2020-09-18        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsPCFvF5NaVs"
      },
      "source": [
        "Copyright © 2019 <a href=\"cognitiveclass.ai\"> cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVqtOZB0NaVs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}